# Multimodal-Sentiment-Analysis-Pipeline-for-YouTube-Videos

### **Text + Audio + Visual Fusion using Deep Learning & Machine Learning**

This repository contains a **complete multimodal sentiment analysis framework** designed to classify YouTube video segments into **Positive, Neutral, and Negative** sentiments using **text, audio, and visual** modalities.
The project evaluates multiple models per modality, applies **10-fold cross-validation**, and uses a **decision-level fusion strategy** with **ELM, SVM, and Random Forest** classifiers.

---

## â­ **Key Features**

* **Tri-modal processing (Text + Audio + Visual)**
* **Transformer-based sentiment models** (BERT, RoBERTa, Wav2Vec2, etc.)
* **Rule-based text models** (VADER, TextBlob)
* **Deep-learning facial emotion recognition** using **DeepFace** and **FER**
* **Acoustic emotional analysis** using Mel spectrograms, MFCCs, and prosodic features
* **Decision-level fusion** that combines probability vectors across modalities
* **Supervised classifiers**:
  âœ“ Extreme Learning Machine (Best performer)
  âœ“ Support Vector Machine
  âœ“ Random Forest
* **10-Fold Cross-Validation** for all models
* **Designed for mental-health trend monitoring**

---

## ğŸ“ **Project Structure**

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_videos/
â”‚   â”œâ”€â”€ transcripts/
â”‚   â”œâ”€â”€ audio_segments/
â”‚   â”œâ”€â”€ frames/
â”‚
â”œâ”€â”€ text_modality/
â”‚
â”œâ”€â”€ audio_modality/
â”‚
â”œâ”€â”€ visual_modality/
â”‚
â”œâ”€â”€ fusion/
â”‚
â”œâ”€â”€ results/
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  **Methodology**

### **1ï¸âƒ£ Data Acquisition**

* YouTube videos downloaded using **yt-dlp**
* Transcripts gathered using **YouTube Transcript API**
* 10 videos â†’ 50 segments (5 seconds each)

---

## **2ï¸âƒ£ Text Modality**

**Models Used**

* DistilBERT (SST-2)
* BERT Multilingual
* RoBERTa-Twitter
* FinBERT
* VADER
* TextBlob

**Process**

* Tokenization â†’ model inference â†’ sentiment mapping
* **Majority voting** generates final text sentiment
* *Best performers*: **TextBlob** & **VADER**

---

## **3ï¸âƒ£ Audio Modality**

**Features Extracted**

* Mel Spectrograms
* MFCCs
* RMS Energy
* Spectral Rolloff
* Zero Crossing Rate

**Models Used**

* **Wav2Vec2-XLSR-53** (Transformer model)
* **Acoustic-ML** (threshold-based classifier)

*Best performer*: **Wav2Vec2-XLSR (82.5% accuracy)**

---

## **4ï¸âƒ£ Visual Modality**

**Frame Processing**

* 10 frames per segment
* OpenCV used for:
  âœ“ Brightness
  âœ“ Contrast
  âœ“ Sharpness

**Models Used**

* **DeepFace (VGG-Face backend)**
* **FER + MTCNN**

*Best performer*: **DeepFace (85% accuracy)**

---

## **5ï¸âƒ£ Multimodal Decision-Level Fusion**

Each modality outputs:

```
[ P(Positive), P(Neutral), P(Negative) ]
```

These are concatenated into a **9-dimensional decision vector**.

**Classifiers Used**

* âœ“ Extreme Learning Machine (ELM) â†’ **100% CV accuracy**
* âœ“ Support Vector Machine (SVM) â†’ **100% CV accuracy**
* âœ“ Random Forest â†’ **98% CV accuracy**
* âœ“ Equal-weight averaging â†’ **92% accuracy**

**Winner:**

### ğŸ¥‡ **Extreme Learning Machine (ELM)**

Fastest, most stable, and most accurate classifier.

---

## ğŸ“Š **Results Summary**

| Modality              | Best Model       | Accuracy |
| --------------------- | ---------------- | -------- |
| **Text**              | TextBlob / VADER | 83â€“82%   |
| **Audio**             | Wav2Vec2-XLSR    | 82.5%    |
| **Visual**            | DeepFace         | 85%      |
| **Multimodal Fusion** | ELM              | **100%** |


Just tell me!
